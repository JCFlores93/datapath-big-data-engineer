{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"19dd7517-adfe-450d-b28e-a023e894605e","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h1>Sesión 1: Introducción Spark </h1> <img src='https://storage.googleapis.com/python-files-datahack/img/Apache_Spark_logo.svg.png' alt='RDD Linage' height='242' width='342'> <ul><strong>Objetivos:</strong> <li>Entender qué es Apache Spark y el funcionamiento</li> <li>Comprender la arquitectura de Spark</li> <li>¿Cómo funciona Spark en AWS, Google Cloud y Azure?</li> <li>Conocer Databricks</li></ul>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h1>Sesión 1: Introducción Spark </h1> <img src='https://storage.googleapis.com/python-files-datahack/img/Apache_Spark_logo.svg.png' alt='RDD Linage' height='242' width='342'> <ul><strong>Objetivos:</strong> <li>Entender qué es Apache Spark y el funcionamiento</li> <li>Comprender la arquitectura de Spark</li> <li>¿Cómo funciona Spark en AWS, Google Cloud y Azure?</li> <li>Conocer Databricks</li></ul>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h1>Sesión 1: Introducción Spark </h1> <img src='https://storage.googleapis.com/python-files-datahack/img/Apache_Spark_logo.svg.png' alt='RDD Linage' height='242' width='342'> <ul><strong>Objetivos:</strong> <li>Entender qué es Apache Spark y el funcionamiento</li> <li>Comprender la arquitectura de Spark</li> <li>¿Cómo funciona Spark en AWS, Google Cloud y Azure?</li> <li>Conocer Databricks</li></ul>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6ee7531b-fb47-45d5-ad63-c2577e4521fe","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Revisemos el video que preparamos con un resumen da Apache Spark <a href='https://youtu.be/hbWEPNuPOs4m'>YouTube</a>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Revisemos el video que preparamos con un resumen da Apache Spark <a href='https://youtu.be/hbWEPNuPOs4m'>YouTube</a>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Revisemos el video que preparamos con un resumen da Apache Spark <a href='https://youtu.be/hbWEPNuPOs4m'>YouTube</a>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a9b39ab7-f2dc-46c7-a94c-07def626f1c0","showTitle":true,"title":"¿Qué es Apache Spark?"}},"outputs":[{"data":{"text/html":["Es un motor computacional unificado y un conjunto de librerías para el procesamiento de data en paralelo. Spark es uno de los proyectos de open source más activos en el mundo. Spark soporta múltiples lenguajes de programación como Python, Java, Scala o R. Posee librerías para diversas tareas como SQL, Streaming y Machine Learning. Puedes ejecutarlo desde en una laptop a miles de servidores. Esto hace que el escalamiento sea simple. [Spark The Definitive Guide, Ch 1]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Es un motor computacional unificado y un conjunto de librerías para el procesamiento de data en paralelo. Spark es uno de los proyectos de open source más activos en el mundo. Spark soporta múltiples lenguajes de programación como Python, Java, Scala o R. Posee librerías para diversas tareas como SQL, Streaming y Machine Learning. Puedes ejecutarlo desde en una laptop a miles de servidores. Esto hace que el escalamiento sea simple. [Spark The Definitive Guide, Ch 1]","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Es un motor computacional unificado y un conjunto de librerías para el procesamiento de data en paralelo. Spark es uno de los proyectos de open source más activos en el mundo. Spark soporta múltiples lenguajes de programación como Python, Java, Scala o R. Posee librerías para diversas tareas como SQL, Streaming y Machine Learning. Puedes ejecutarlo desde en una laptop a miles de servidores. Esto hace que el escalamiento sea simple. [Spark The Definitive Guide, Ch 1]\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"77f6aee2-7043-4918-a9f6-1d68b76cef25","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<img src='https://storage.googleapis.com/python-files-datahack/img/spark%20toolset.png' alt='RDD Linage' height='642' width='642'> "]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<img src='https://storage.googleapis.com/python-files-datahack/img/spark%20toolset.png' alt='RDD Linage' height='642' width='642'> ","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<img src='https://storage.googleapis.com/python-files-datahack/img/spark%20toolset.png' alt='RDD Linage' height='642' width='642'> \")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"605b734d-87bd-44f9-9059-6a76fc73d046","showTitle":true,"title":"Disgreguemos el concepto de Apache Spark"}},"outputs":[{"data":{"text/html":["<b>Unificado</b>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<b>Unificado</b>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<b>Unificado</b>\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d1474f29-91da-4fbe-8df7-e204477aa42f","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Spark está diseñado para soportar un gran rango de tareas de data analytics como por ejemplo una carga de información, ejecución de querys en SQL, algoritmos de Machine Learning y flujos en Streaming. Por jemplo si cargamos data usando una query SQL y luego evaluamos un modelo de Machine Learning sobre ese dataset usando las librerías de Spark, el engine puede combinar esos pasos para que no se tenga escanear dos veces la data"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Spark está diseñado para soportar un gran rango de tareas de data analytics como por ejemplo una carga de información, ejecución de querys en SQL, algoritmos de Machine Learning y flujos en Streaming. Por jemplo si cargamos data usando una query SQL y luego evaluamos un modelo de Machine Learning sobre ese dataset usando las librerías de Spark, el engine puede combinar esos pasos para que no se tenga escanear dos veces la data","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Spark está diseñado para soportar un gran rango de tareas de data analytics como por ejemplo una carga de información, ejecución de querys en SQL, algoritmos de Machine Learning y flujos en Streaming. Por jemplo si cargamos data usando una query SQL y luego evaluamos un modelo de Machine Learning sobre ese dataset usando las librerías de Spark, el engine puede combinar esos pasos para que no se tenga escanear dos veces la data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"dccec065-3cb9-473c-a49d-662cfe9b5478","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<b>Motor computacional</b>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<b>Motor computacional</b>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<b>Motor computacional</b>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"dfb55c13-92f8-4121-8fdd-9b71ee2e7d72","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Spark se centra en el procesamiento, tomando data de cloud storage systems como Google Cloud Storage, Azure Storage o Amazon S3, además de otros tipos de storage como Apache Hadoop, MySQL o Apache Kafka. Esto significa que Spark no almacena data. Es decir existe una separación entre el procesamiento y almacenamiento caso contrario a lo que suscedía con Apache Hadoop"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Spark se centra en el procesamiento, tomando data de cloud storage systems como Google Cloud Storage, Azure Storage o Amazon S3, además de otros tipos de storage como Apache Hadoop, MySQL o Apache Kafka. Esto significa que Spark no almacena data. Es decir existe una separación entre el procesamiento y almacenamiento caso contrario a lo que suscedía con Apache Hadoop","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Spark se centra en el procesamiento, tomando data de cloud storage systems como Google Cloud Storage, Azure Storage o Amazon S3, además de otros tipos de storage como Apache Hadoop, MySQL o Apache Kafka. Esto significa que Spark no almacena data. Es decir existe una separación entre el procesamiento y almacenamiento caso contrario a lo que suscedía con Apache Hadoop\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"efc56c9d-3bf4-47e4-a791-ce9cec2402d5","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<b>Librerías</b>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<b>Librerías</b>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<b>Librerías</b>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1a611f23-7b74-416b-98a8-af0cd759613f","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Busca proveer una API unificada para tareas comunes de análisis. Las librerías permiten agregar a Spark nuevas funcionalidades. Por ejemplo librerías para SQL y data estructurada SparkSQL, Machine Learning (MLlib y SparkML), stream processing (Structured Streaming) y análisis de gráfos con GraphX. Además existen muchas más librerías que puedes ver en https://spark-packages.org/ <b>¿Cuántos paquetes tiene disponible hoy Spark? ¿Cuál es la categoría más popular? </b> "]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Busca proveer una API unificada para tareas comunes de análisis. Las librerías permiten agregar a Spark nuevas funcionalidades. Por ejemplo librerías para SQL y data estructurada SparkSQL, Machine Learning (MLlib y SparkML), stream processing (Structured Streaming) y análisis de gráfos con GraphX. Además existen muchas más librerías que puedes ver en https://spark-packages.org/ <b>¿Cuántos paquetes tiene disponible hoy Spark? ¿Cuál es la categoría más popular? </b> ","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Busca proveer una API unificada para tareas comunes de análisis. Las librerías permiten agregar a Spark nuevas funcionalidades. Por ejemplo librerías para SQL y data estructurada SparkSQL, Machine Learning (MLlib y SparkML), stream processing (Structured Streaming) y análisis de gráfos con GraphX. Además existen muchas más librerías que puedes ver en https://spark-packages.org/ <b>¿Cuántos paquetes tiene disponible hoy Spark? ¿Cuál es la categoría más popular? </b> \")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d5b1e1fb-5bdf-48e0-be89-c244064f5020","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<img src='https://storage.googleapis.com/python-files-datahack/img/sparkPackage.png' alt='RDD Linage' height='642' width='642'> "]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<img src='https://storage.googleapis.com/python-files-datahack/img/sparkPackage.png' alt='RDD Linage' height='642' width='642'> ","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<img src='https://storage.googleapis.com/python-files-datahack/img/sparkPackage.png' alt='RDD Linage' height='642' width='642'> \")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cf7a91dc-5af7-4be3-badf-3479627cc02e","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>Brieft Apache Spark History</h2>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>Brieft Apache Spark History</h2>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>Brieft Apache Spark History</h2>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ad59815a-7162-4e39-b3f2-ff98a37720f6","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Apache Spark empezo en el 2009 en UC Berkeley con un <a href='https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf'>paper publicado</a> como parte del Spark research project. En esa epoca Hadoop como MapReduce era el motor dominante. En el 2011 desarrollaron Shark que permitio a Spark ejecutar SQL. En el 2013 paso a ser un proyecto parte de la Apache Software Foundation. Parte del equipo de AMPlab fundo Databricks para darle mayor soporte al proyecto."]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Apache Spark empezo en el 2009 en UC Berkeley con un <a href='https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf'>paper publicado</a> como parte del Spark research project. En esa epoca Hadoop como MapReduce era el motor dominante. En el 2011 desarrollaron Shark que permitio a Spark ejecutar SQL. En el 2013 paso a ser un proyecto parte de la Apache Software Foundation. Parte del equipo de AMPlab fundo Databricks para darle mayor soporte al proyecto.","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Apache Spark empezo en el 2009 en UC Berkeley con un <a href='https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf'>paper publicado</a> como parte del Spark research project. En esa epoca Hadoop como MapReduce era el motor dominante. En el 2011 desarrollaron Shark que permitio a Spark ejecutar SQL. En el 2013 paso a ser un proyecto parte de la Apache Software Foundation. Parte del equipo de AMPlab fundo Databricks para darle mayor soporte al proyecto.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"200e29a5-7f34-49a1-b87e-9cdb3e07ab49","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Apache Spark 1.0 salio en el 2014, Apache Spark 2.0 en 2016 y Apache 3.0 en 2020."]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Apache Spark 1.0 salio en el 2014, Apache Spark 2.0 en 2016 y Apache 3.0 en 2020.","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Apache Spark 1.0 salio en el 2014, Apache Spark 2.0 en 2016 y Apache 3.0 en 2020.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"00a230c5-9703-442e-9802-f0c0ae9cc685","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>Spark's Architecture</h2>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>Spark's Architecture</h2>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>Spark's Architecture</h2>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7da7f81f-8841-470e-a08f-782407c4965f","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Un cluster o grupo de computadoras, agrupa los recursos como RAM y CPUs permitiendonos una mayor capacidad. Para que podamos sacarle el mayor provecho las maquinas agrupadas no son sufiente, necesitan un framework para coordinar el trabajo. Spark hace eso, administra y coordina la ejecucion de las tareas sobre la data para todo el cluster"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Un cluster o grupo de computadoras, agrupa los recursos como RAM y CPUs permitiendonos una mayor capacidad. Para que podamos sacarle el mayor provecho las maquinas agrupadas no son sufiente, necesitan un framework para coordinar el trabajo. Spark hace eso, administra y coordina la ejecucion de las tareas sobre la data para todo el cluster","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Un cluster o grupo de computadoras, agrupa los recursos como RAM y CPUs permitiendonos una mayor capacidad. Para que podamos sacarle el mayor provecho las maquinas agrupadas no son sufiente, necesitan un framework para coordinar el trabajo. Spark hace eso, administra y coordina la ejecucion de las tareas sobre la data para todo el cluster\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0080f807-568a-4571-a0a0-7f2999364083","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Las maquinas que Spark usa para ejecutar las tareas son administradas por un cluster manager como: Spark's standalone cluster manager, YARN, Mesos o Kubernetes. Nosotros luego enviamos un <b>Spark Application</b> a cualquiera de esos cluster managers, los cuales nos asignaran recursos para nuestra aplicacion y asi puedan hacer su trabajo."]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Las maquinas que Spark usa para ejecutar las tareas son administradas por un cluster manager como: Spark's standalone cluster manager, YARN, Mesos o Kubernetes. Nosotros luego enviamos un <b>Spark Application</b> a cualquiera de esos cluster managers, los cuales nos asignaran recursos para nuestra aplicacion y asi puedan hacer su trabajo.","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Las maquinas que Spark usa para ejecutar las tareas son administradas por un cluster manager como: Spark's standalone cluster manager, YARN, Mesos o Kubernetes. Nosotros luego enviamos un <b>Spark Application</b> a cualquiera de esos cluster managers, los cuales nos asignaran recursos para nuestra aplicacion y asi puedan hacer su trabajo.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"29a38ee9-afac-4d97-bc7f-260c366437bc","showTitle":true,"title":"Spark Applications"}},"outputs":[{"data":{"text/html":["Un Spark Application consiste de un <b>driver process</b> y un conjunto de <b>executor processes</b>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Un Spark Application consiste de un <b>driver process</b> y un conjunto de <b>executor processes</b>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Un Spark Application consiste de un <b>driver process</b> y un conjunto de <b>executor processes</b>. El procesos driver contienen la funcion main()\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"425c0ced-280d-4fd4-8fe4-9ed6baac33f3","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<ul><strong>El proceso driver es responsable de tres actividades:</strong> <li>Mantener la infomacion de acerca del Spark Application</li> <li>Responder al programa del usuario o input</li> <li>Analizar y distribuir, y programar el trabajo a los executors</li></ul>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<ul><strong>El proceso driver es responsable de tres actividades:</strong> <li>Mantener la infomacion de acerca del Spark Application</li> <li>Responder al programa del usuario o input</li> <li>Analizar y distribuir, y programar el trabajo a los executors</li></ul>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<ul><strong>El proceso driver es responsable de tres actividades:</strong> <li>Mantener la infomacion de acerca del Spark Application</li> <li>Responder al programa del usuario o input</li> <li>Analizar y distribuir, y programar el trabajo a los executors</li></ul>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d7b1d6ba-7b20-414b-b37e-d47aba829bf4","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["El Driver Process es el corazon de un Apache Spark Application y mantiene la informacion relevante durante la vida de la aplicacion."]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"El Driver Process es el corazon de un Apache Spark Application y mantiene la informacion relevante durante la vida de la aplicacion.","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"El Driver Process es el corazon de un Apache Spark Application y mantiene la informacion relevante durante la vida de la aplicacion.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b01e5268-3f07-48ea-bed4-d5ae6bbd460b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Los Executor Process son responsables de ejecutar el codigo asignado por el Driver y reportar el estado de su ejecucion al driver. "]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Los Executor Process son responsables de ejecutar el codigo asignado por el Driver y reportar el estado de su ejecucion al driver. ","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Los Executor Process son responsables de ejecutar el codigo asignado por el Driver y reportar el estado de su ejecucion al driver. \")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ee55ae50-8cb5-4cbb-a1d2-4bfb189f1b5c","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<img src='https://storage.googleapis.com/python-files-datahack/img/architecture.png' alt='RDD Linage' height='442' width='542'>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<img src='https://storage.googleapis.com/python-files-datahack/img/architecture.png' alt='RDD Linage' height='442' width='542'>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<img src='https://storage.googleapis.com/python-files-datahack/img/architecture.png' alt='RDD Linage' height='442' width='542'>\")\n","\n","tabla a 7 mil millones de filas 1tb o más                    \n","antonio Peru\n","juan Chile\n","Maria Alemania\n","\n","tabla b 190 filas/paises 10mb\n","Peru America\n","Chile America\n","Alemania Europa"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2bad5b5a-ee98-4d8e-86e2-526045b902e9","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Considerar que pueden haber varios Spark Application ejecutandose en un cluster y siendo administrados por un Cluster Manager."]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Considerar que pueden haber varios Spark Application ejecutandose en un cluster y siendo administrados por un Cluster Manager.","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Considerar que pueden haber varios Spark Application ejecutandose en un cluster y siendo administrados por un Cluster Manager.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3baef58b-29bd-4942-9945-a93ad1b64b27","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Las APIs de Spark hacen posible correr Spark usando varios lenguajes de programación. Este código es traducido en 'código' Spark que se ejecuta en el cluster."]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Las APIs de Spark hacen posible correr Spark usando varios lenguajes de programación. Este código es traducido en 'código' Spark que se ejecuta en el cluster.","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Las APIs de Spark hacen posible correr Spark usando varios lenguajes de programación. Este código es traducido en 'código' Spark que se ejecuta en el cluster.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"95e3d8ae-75a8-40c6-94fe-6b442a40b766","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Spark soporta varios lenguajes de programación como: Scala (lenguaje por dejecto), Java, Python, SQL y R."]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Spark soporta varios lenguajes de programación como: Scala (lenguaje por dejecto), Java, Python, SQL y R.","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Spark soporta varios lenguajes de programación como: Scala (lenguaje por dejecto), Java, Python, SQL y R.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"979a5e27-271e-4e9a-a91e-617c341202c6","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<img src='https://storage.googleapis.com/python-files-datahack/img/Apache-Spark-Architecture.jpg' alt='RDD Linage' height='442' width='542'>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<img src='https://storage.googleapis.com/python-files-datahack/img/Apache-Spark-Architecture.jpg' alt='RDD Linage' height='442' width='542'>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<img src='https://storage.googleapis.com/python-files-datahack/img/Apache-Spark-Architecture.jpg' alt='RDD Linage' height='442' width='542'>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e7865d23-48cb-4973-8676-0b47ca914653","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>¿Por qué Scala?</h2>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>¿Por qué Scala?</h2>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>¿Por qué Scala?</h2>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"343e8c91-2d3b-4f80-be3f-f8d6c08d3bc0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Tomemos unos minutos para leer y opinar: https://www.quora.com/Why-is-Apache-Spark-implemented-in-Scala"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Tomemos unos minutos para leer y opinar: https://www.quora.com/Why-is-Apache-Spark-implemented-in-Scala","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Tomemos unos minutos para leer y opinar: https://www.quora.com/Why-is-Apache-Spark-implemented-in-Scala\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3d488b96-1d8f-496a-8636-ae062234c777","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>APIs Principales en Apache Spark</h2>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>APIs Principales en Apache Spark</h2>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>APIs Principales en Apache Spark</h2>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ecd46b1a-6c7e-44ab-8630-c7479f456deb","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Spark tiene dos APIs fundamentales: Una <b>No Estructurada</b> que es de bajo nivel y otra <b>Estructurada</b> que es de alto nivel"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Spark tiene dos APIs fundamentales: Una <b>No Estructurada</b> que es de bajo nivel y otra <b>Estructurada</b> que es de alto nivel","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Spark tiene dos APIs fundamentales: Una <b>No Estructurada</b> que es de bajo nivel y otra <b>Estructurada</b> que es de alto nivel\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"003170d9-e202-4a44-8f4b-a31690547856","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>¿Qué es un Spark Session?</h2>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>¿Qué es un Spark Session?</h2>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>¿Qué es un Spark Session?</h2>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e6c0fba4-ade0-4069-9996-b089f5874546","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Es un proceso que se ejecuta en el driver para controlar tu Spark Application. Es la manera como Spark ejecuta las transformaciones definidas por el usuario en el cluster. Siempre hay una correspondencia de uno a uno entre un SparkSession y un SparkApplication"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Es un proceso que se ejecuta en el driver para controlar tu Spark Application. Es la manera como Spark ejecuta las transformaciones definidas por el usuario en el cluster. Siempre hay una correspondencia de uno a uno entre un SparkSession y un SparkApplication","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Es un proceso que se ejecuta en el driver para controlar tu Spark Application. Es la manera como Spark ejecuta las transformaciones definidas por el usuario en el cluster. Siempre hay una correspondencia de uno a uno entre un SparkSession y un SparkApplication\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5e125ec8-5c10-4bd5-b0ed-d9c484c98bb5","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[6]: </div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[6]: </div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"},{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"/?o=983841310995410#setting/sparkui/0204-000317-omits254/driver-4094833824504581317\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.0.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[8]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Databricks Shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=983841310995410#setting/sparkui/0204-000317-omits254/driver-4094833824504581317\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["#Tiempo de código!, ejecuta las siguientes línea por favor\n","spark"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"770ef697-9af5-49f3-b7f3-301df432c3fe","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["dfDemo = spark.range(100).toDF(\"number\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d9c5d558-4bae-4c72-af6b-306cdbef9d87","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>¿Qué es un DataFrame?</h2>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>¿Qué es un DataFrame?</h2>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>¿Qué es un DataFrame?</h2>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"24f9207f-d523-4ec4-9421-9e6cbe0edc8e","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Es el APÏ estructurada más popular (recordemos que teníamos dos), y en simple representa una tabla de datos con filas y columnas. Existe una lista que define los nombres de columnas y tipos de datos que se le conoce como schema"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Es el APÏ estructurada más popular (recordemos que teníamos dos), y en simple representa una tabla de datos con filas y columnas. Existe una lista que define los nombres de columnas y tipos de datos que se le conoce como schema","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Es el APÏ estructurada más popular (recordemos que teníamos dos), y en simple representa una tabla de datos con filas y columnas. Existe una lista que define los nombres de columnas y tipos de datos que se le conoce como schema\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b69da6b5-762b-4aec-a9ce-a3328ee028b4","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>¿Qué son las Particiones? y ¿Cómo Spark la usa para su paralelismo?</h2>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>¿Qué son las Particiones? y ¿Cómo Spark la usa para su paralelismo?</h2>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>¿Qué son las Particiones? y ¿Cómo Spark la usa para su paralelismo?</h2>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f021b56b-c5ce-4d95-9454-6e6eb365f6d3","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Recordemos que los que realizan las operaciones, transformaciones, agregaciones son los executors, para que ellos tengan data que procesar Spark rompe la data en particiones. Se considera una partició a una colección de filas que se encuentra físicamente en un nodo."]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Recordemos que los que realizan las operaciones, transformaciones, agregaciones son los executors, para que ellos tengan data que procesar Spark rompe la data en particiones. Se considera una partició a una colección de filas que se encuentra físicamente en un nodo.","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Recordemos que los que realizan las operaciones, transformaciones, agregaciones son los executors, para que ellos tengan data que procesar Spark rompe la data en particiones. Se considera una partició a una colección de filas que se encuentra físicamente en un nodo.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"39c9ae47-f2a5-429a-acd6-dbb3f4ddf752","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<ul><strong>Analisemos y respondamos</strong> <li>Si se tiene una partición y 100 executors, ¿Existe paralelismo?</li> <li>Si se tiene 1 executor y 100 particiones ¿Existe paralelismo?</li>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<ul><strong>Analisemos y respondamos</strong> <li>Si se tiene una partición y 100 executors, ¿Existe paralelismo?</li> <li>Si se tiene 1 executor y 100 particiones ¿Existe paralelismo?</li>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<ul><strong>Analisemos y respondamos</strong> <li>Si se tiene una partición y 100 executors, ¿Existe paralelismo?</li> <li>Si se tiene 1 executor y 100 particiones ¿Existe paralelismo?</li>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5d2584b7-5fa5-4966-8f88-e162a5c560a0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[8]: 8</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[8]: 8</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#Veamos cuantas particiones por defecto tiene el cluster\n","spark.sparkContext.defaultParallelism"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a2fc69ae-d1de-4c65-a6a7-a3d4688264a5","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<p> <strong>Buena practica: </strong>es trabajar con un número de particiones mínimo igual al número de núcleos que tiene el cluster.</p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<p> <strong>Buena practica: </strong>es trabajar con un número de particiones mínimo igual al número de núcleos que tiene el cluster.</p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<p> <strong>Buena practica: </strong>es trabajar con un número de particiones mínimo igual al número de núcleos que tiene el cluster.</p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"70ff3d74-31ed-42ce-acaf-96f4d360d373","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">[1, 2, 3, 4, 5, 6, 7, 8]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">[1, 2, 3, 4, 5, 6, 7, 8]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#Veamos cómo particionar en Spark\n","\n","#Python genero un vector\n","data = [1,2,3,4,5,6,7,8,9,10] #python puro\n","\n","#\"Subo\" a Spark con dos particiones\n","rdds = spark.sparkContext.parallelize(data,2)#definimos 2 particiones\n","\n","#\"Bajo\" a Python para imprimir los resultados\n","print(rdds.take(8))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d4b2111f-92e9-48d9-88d9-53fd54256527","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[31]: 2</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[31]: 2</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["rdds.getNumPartitions()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d8e4d5c6-cdda-425f-ae1d-1ee507154d2a","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>Principales operaciones en Apache Spark</h2>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>Principales operaciones en Apache Spark</h2>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>Principales operaciones en Apache Spark</h2>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5cfc40d4-6d07-4e5e-afed-b3795da981e6","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h3>Transformaciones</h3>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h3>Transformaciones</h3>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h3>Transformaciones</h3>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ae03de6e-ffa8-48ba-9a18-fddc0bf9a953","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Las estructuras de datos en Spark son <b>Inmutables</b>, lo que siginifica que no pueden ser cambiados una vez que se crean. Para 'cambiarlo' necesitamos escribir código para que Spark sepa cómo lo queremos variar. Estas instrucciones se llaman <b>Transformaciones</b>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Las estructuras de datos en Spark son <b>Inmutables</b>, lo que siginifica que no pueden ser cambiados una vez que se crean. Para 'cambiarlo' necesitamos escribir código para que Spark sepa cómo lo queremos variar. Estas instrucciones se llaman <b>Transformaciones</b>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Las estructuras de datos en Spark son <b>Inmutables</b>, lo que siginifica que no pueden ser cambiados una vez que se crean. Para 'cambiarlo' necesitamos escribir código para que Spark sepa cómo lo queremos variar. Estas instrucciones se llaman <b>Transformaciones</b>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1671dd26-ec6b-471c-a179-89debabf529d","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[{"name":"mayorCien","schema":{"fields":[{"metadata":{},"name":"number","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null,"typeStr":"pyspark.sql.dataframe.DataFrame"}],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["mayorCien = dfDemo.where(\"number > 100 \") #¿Obtuvimos algún resultado? ¿Cómo lo harías con Pandas?\n","#df2"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"856729b3-5ad1-495a-b1c0-356f794a6370","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["Spark siempre esperará una <b>Acción</b> para proceder a ejecutar todas las transformaciones"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Spark siempre esperará una <b>Acción</b> para proceder a ejecutar todas las transformaciones","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"Spark siempre esperará una <b>Acción</b> para proceder a ejecutar todas las transformaciones. Existen dos tipos de transformaciones: Narrow Transformation y Wide Transformations\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1ebc6a9a-6d39-43c1-8596-a3a3507e6839","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h4>Narrow Transformation</h4><p>Son el resultado de operaciones donde la data es tomada de una sola partición, es decir no depende de otras particiones para ejecutarse. Podemos encontrar funciones como map y filter.</p> <p>Spark agrupa este tipo de transformaciones como un 'stage' llamado <strong>pipelining</strong></p> <p><img src='https://storage.googleapis.com/python-files-datahack/img/NarrowTr.png' alt='RDD Linage' height='442' width='542'></p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h4>Narrow Transformation</h4><p>Son el resultado de operaciones donde la data es tomada de una sola partición, es decir no depende de otras particiones para ejecutarse. Podemos encontrar funciones como map y filter.</p> <p>Spark agrupa este tipo de transformaciones como un 'stage' llamado <strong>pipelining</strong></p> <p><img src='https://storage.googleapis.com/python-files-datahack/img/NarrowTr.png' alt='RDD Linage' height='442' width='542'></p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h4>Narrow Transformation</h4><p>Son el resultado de operaciones donde la data es tomada de una sola partición, es decir no depende de otras particiones para ejecutarse. Podemos encontrar funciones como map y filter.</p> <p>Spark agrupa este tipo de transformaciones como un 'stage' llamado <strong>pipelining</strong></p> <p><img src='https://storage.googleapis.com/python-files-datahack/img/NarrowTr.png' alt='RDD Linage' height='442' width='542'></p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a71954ba-d1aa-48aa-94fb-042b646c85ae","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Map: </strong> ejecuta una función a cada elemento del RDD, retornando un RDD conteniendo listas teniendo dentro el producto de la función recibida. </strong> "]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Map: </strong> ejecuta una función a cada elemento del RDD, retornando un RDD conteniendo listas teniendo dentro el producto de la función recibida. </strong> ","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Map: </strong> ejecuta una función a cada elemento del RDD, retornando un RDD conteniendo listas teniendo dentro el producto de la función recibida. </strong> \")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7576d368-65e4-460e-b549-931bd8ce2f23","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[10]: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 224, 178]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[10]: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 224, 178]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#Python\n","data2 = [1, 2, 3, 4, 5, 6 , 7, 8, 9, 10, 112, 89]\n","\n","#Spark Transformation\n","distData2 = spark.sparkContext.parallelize(data2)\n","distData3 = distData2.map(lambda x:x*2)#narrow transformation map\n","\n","#Spark Action\n","distData3.collect()#Action"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4ea0ac28-90ca-4b7a-8387-a3ea6567fce9","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">1 \n","4 \n","9 \n","16 \n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">1 \n4 \n9 \n16 \n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["nums = sc.parallelize([1, 2, 3, 4])\n","squared = nums.map(lambda x: x * x).collect()#se recibe un iterador\n","for num in squared:\n","    print(\"%i \" % (num))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7558cefe-703f-4d6f-9316-026d8801addd","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Filter: </strong> Filtra los elementos de un RDD"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Filter: </strong> Filtra los elementos de un RDD","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Filter: </strong> Filtra los elementos de un RDD\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c55e0664-008f-4184-94d5-c33d1ed6df3e","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[12]: [0, 2]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[12]: [0, 2]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#Filtremos los elementos del RDD que no son múltiplos de 2\n","\n","#Python genero número aleatorios\n","dataFilter = range(100000)\n","dataFilter\n","\n","#Spark Transformations\n","distDataFilter3 = spark.sparkContext.parallelize(dataFilter)\n","distDataFilter4 = distDataFilter3.filter(lambda x:x%2==0)\n","\n","#Spark Actions\n","distDataFilter4.take(2)#Action\n","#distDataFilter4.take(10)#Action"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d2e9384c-fbf8-40ef-93d3-49912f57a36b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Union: </strong> Sirve para unificar RDDs, si hay duplicados por defecto los mantiene. </strong>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Union: </strong> Sirve para unificar RDDs, si hay duplicados por defecto los mantiene. </strong>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Union: </strong> Sirve para unificar RDDs, si hay duplicados por defecto los mantiene. </strong>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3bbb7765-214c-47bf-80f3-c796051f5740","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[13]: [1, &#39;ene&#39;, 2016, 3, &#39;ene&#39;, 2019, 7, &#39;oct&#39;]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[13]: [1, &#39;ene&#39;, 2016, 3, &#39;ene&#39;, 2019, 7, &#39;oct&#39;]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["data = [1,\"ene\",2016]\n","data2 = [3,\"ene\",2019]\n","data3 = [7,\"oct\",2020]\n","d1 = spark.sparkContext.parallelize(data)\n","d2 = spark.sparkContext.parallelize(data2)\n","d3 = spark.sparkContext.parallelize(data3)\n","rddUnion = d1.union(d2)\n","rddUnionf = rddUnion.union(d3)\n","rddUnionf.take(8)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2cbd1fe2-6975-4c47-8888-0412b4b43abb","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>Wide Transformation</h2><p>Son el resultado de operaciones donde la data es tomada de direfentes particiones, es decir depende de data de otras particiones para ejecutarse. Podemos encontrar funciones como groupByKey y reduceByKey. Las tuplas con la misma llave deben terminar en la misma partifición, esto significa que Spark debe ejecutar un RDD shuffle lo cual transfiere/mueve data a través del cluster resultando un nuevo stage con un nuevo grupo de particiones.</p><img src='https://storage.googleapis.com/python-files-datahack/img/WideTr.png' alt='RDD Linage' height='442' width='342'>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>Wide Transformation</h2><p>Son el resultado de operaciones donde la data es tomada de direfentes particiones, es decir depende de data de otras particiones para ejecutarse. Podemos encontrar funciones como groupByKey y reduceByKey. Las tuplas con la misma llave deben terminar en la misma partifición, esto significa que Spark debe ejecutar un RDD shuffle lo cual transfiere/mueve data a través del cluster resultando un nuevo stage con un nuevo grupo de particiones.</p><img src='https://storage.googleapis.com/python-files-datahack/img/WideTr.png' alt='RDD Linage' height='442' width='342'>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>Wide Transformation</h2><p>Son el resultado de operaciones donde la data es tomada de direfentes particiones, es decir depende de data de otras particiones para ejecutarse. Podemos encontrar funciones como groupByKey y reduceByKey. Las tuplas con la misma llave deben terminar en la misma partifición, esto significa que Spark debe ejecutar un RDD shuffle lo cual transfiere/mueve data a través del cluster resultando un nuevo stage con un nuevo grupo de particiones.</p><img src='https://storage.googleapis.com/python-files-datahack/img/WideTr.png' alt='RDD Linage' height='442' width='342'>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c336e1e5-e36f-475c-9c46-7d44db18e34a","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Join: </strong> Unimos dos key/value RDDs </strong>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Join: </strong> Unimos dos key/value RDDs </strong>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Join: </strong> Unimos dos key/value RDDs </strong>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"de708a9a-9c03-4cec-843d-a5c81845997f","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">[(&#39;b&#39;, (2, 7)), (&#39;c&#39;, (3, 3)), (&#39;c&#39;, (3, 8)), (&#39;A&#39;, (1, 4)), (&#39;A&#39;, (1, 6))]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">[(&#39;b&#39;, (2, 7)), (&#39;c&#39;, (3, 3)), (&#39;c&#39;, (3, 8)), (&#39;A&#39;, (1, 4)), (&#39;A&#39;, (1, 6))]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#Inner join\n","data = sc.parallelize([('A',1),('b',2),('c',3),('k',9)])\n","data2 =sc.parallelize([('A',4),('A',6),('b',7),('c',3),('c',8),('m',13)])\n","result = data.join(data2)#por defecto es un innerjoin\n","print(result.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0fd0bc2c-117d-44fc-a560-c98d31066e46","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">[(&#39;b&#39;, ([2, 4], 7)), (&#39;c&#39;, (3, 3)), (&#39;c&#39;, (3, 8)), (&#39;m&#39;, (None, 13)), (&#39;A&#39;, (1, 4)), (&#39;A&#39;, (1, 6))]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">[(&#39;b&#39;, ([2, 4], 7)), (&#39;c&#39;, (3, 3)), (&#39;c&#39;, (3, 8)), (&#39;m&#39;, (None, 13)), (&#39;A&#39;, (1, 4)), (&#39;A&#39;, (1, 6))]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#rightOuterJoin\n","#Devuelve una tupla (key, value) en caso de Python si no encuenra el valor None es usado\n","data = sc.parallelize([('A',1),('b',[2,4]),('c',3),('k',9)])\n","data2 =sc.parallelize([('A',4),('A',6),('b',7),('c',3),('c',8),('m',13)])\n","result = data.rightOuterJoin(data2)\n","print(result.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d38375f9-aef4-49d8-8b88-4bcbdc85e0b0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">[(&#39;k&#39;, (9, None)), (&#39;b&#39;, (2, 7)), (&#39;c&#39;, ([3, &#39;pyme&#39;], 3)), (&#39;c&#39;, ([3, &#39;pyme&#39;], 8)), (&#39;A&#39;, (1, 4)), (&#39;A&#39;, (1, 6))]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">[(&#39;k&#39;, (9, None)), (&#39;b&#39;, (2, 7)), (&#39;c&#39;, ([3, &#39;pyme&#39;], 3)), (&#39;c&#39;, ([3, &#39;pyme&#39;], 8)), (&#39;A&#39;, (1, 4)), (&#39;A&#39;, (1, 6))]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#leftOuterJoin\n","data = sc.parallelize([('A',1),('b',2),('c',[3,'pyme']),('k',9)])\n","data2 =sc.parallelize([('A',4),('A',6),('b',7),('c',3),('c',8),('m',13)])\n","result = data.leftOuterJoin(data2)\n","print(result.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"240ffc0c-9818-412f-9886-91a62bc2c8cf","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">[(&#39;k&#39;, (9, None)), (&#39;b&#39;, (2, 7)), (&#39;c&#39;, ([3, &#39;pyme&#39;], 3)), (&#39;c&#39;, ([3, &#39;pyme&#39;], 8)), (&#39;m&#39;, (None, 13)), (&#39;A&#39;, (1, 4)), (&#39;A&#39;, (1, 6))]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">[(&#39;k&#39;, (9, None)), (&#39;b&#39;, (2, 7)), (&#39;c&#39;, ([3, &#39;pyme&#39;], 3)), (&#39;c&#39;, ([3, &#39;pyme&#39;], 8)), (&#39;m&#39;, (None, 13)), (&#39;A&#39;, (1, 4)), (&#39;A&#39;, (1, 6))]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#fullOuterJoin\n","data = sc.parallelize([('A',1),('b',2),('c',[3,'pyme']),('k',9)])\n","data2 =sc.parallelize([('A',4),('A',6),('b',7),('c',3),('c',8),('m',13)])\n","result = data.fullOuterJoin(data2)\n","print(result.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"32161e6b-cfb1-4630-893a-b4b7d6ca711e","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Intersection: </strong> Retorna los elemento en común de dos RDDs sin duplicados. La ejecución hace un mayor consumo de recursos por que requiere de <i>'shuffle'</i> a través de la red</strong>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Intersection: </strong> Retorna los elemento en común de dos RDDs sin duplicados. La ejecución hace un mayor consumo de recursos por que requiere de <i>'shuffle'</i> a través de la red</strong>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Intersection: </strong> Retorna los elemento en común de dos RDDs sin duplicados. La ejecución hace un mayor consumo de recursos por que requiere de <i>'shuffle'</i> a través de la red</strong>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"38679b30-924c-4003-8c86-22e23c27756c","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[66]: [(1, &#39;jan&#39;, 2016)]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[66]: [(1, &#39;jan&#39;, 2016)]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["rdd1 = spark.sparkContext.parallelize([(1,\"jan\",2016),(3,\"nov\",2014, (16,\"feb\",2014))])\n","rdd2 = sc.parallelize([(5,\"dec\",2014),(1,\"jan\",2016)])\n","comman = rdd1.intersection(rdd2)\n","comman.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3ba1d90e-0f92-4359-9bb6-1910dff752de","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Distinct: </strong> Retorna los elementos distintos de un RDD"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Distinct: </strong> Retorna los elementos distintos de un RDD","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Distinct: </strong> Retorna los elementos distintos de un RDD\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6902daa8-c81e-4a1f-aa5e-d8c09243cc06","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">[(1, &#39;jan&#39;, 2016), (16, &#39;feb&#39;, 2014), (3, &#39;nov&#39;, 2014)]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">[(1, &#39;jan&#39;, 2016), (16, &#39;feb&#39;, 2014), (3, &#39;nov&#39;, 2014)]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["rdd1 = sc.parallelize([(1,\"jan\",2016),(3,\"nov\",2014),(16,\"feb\",2014),(3,\"nov\",2014)])\n","result = rdd1.distinct()\n","print(result.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d72b0164-26e2-4219-8027-db26ba3fa562","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>1.2.2.4 groupByKey: </strong> Retorna los elementos de un <key,value> RDD agrupados por la key. Se recomienda por optimización usar <strong>reduceByKey</strong> en los casos que sea posible. (https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)<p><img src='https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/group_by.png' alt='RDD Linage' height='280' width='442'></p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>1.2.2.4 groupByKey: </strong> Retorna los elementos de un <key,value> RDD agrupados por la key. Se recomienda por optimización usar <strong>reduceByKey</strong> en los casos que sea posible. (https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)<p><img src='https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/group_by.png' alt='RDD Linage' height='280' width='442'></p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>1.2.2.4 groupByKey: </strong> Retorna los elementos de un <key,value> RDD agrupados por la key. Se recomienda por optimización usar <strong>reduceByKey</strong> en los casos que sea posible. (https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)<p><img src='https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/group_by.png' alt='RDD Linage' height='280' width='442'></p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e54e3bdd-a421-444e-a885-cfe6395d6717","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">k [5, 6]\n","s [3, 4]\n","t [8]\n","p [7, 5]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">k [5, 6]\ns [3, 4]\nt [8]\np [7, 5]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["part = 3 #TIP: podemos definir los numeros de particiones \n","data = sc.parallelize([('k',5),('s',3),('s',4),('p',7),('p',5),('t',8),('k',6)],3)\n","group = data.groupByKey(part).collect()\n","for t in group:\n","\tprint(t[0],[v for v in t[1]])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"09e74887-41bd-43ce-9a6a-0626618ca515","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["x = sc.parallelize([\n","    (\"PE\", 1), (\"PE\", \"🐮\"),(\"India\", 1),\n","    (\"🐮\", 1), (\"India\", 4), (\"India\", 9),\n","    (\"PE\", 8), (\"PE\", 3), (\"India\", 4),\n","    (\"🐮\", 6), (\"🐮\", 9), (\"🐮\", 5)], 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"95a5720e-1445-4481-9c31-84f2c7427088","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[72]: [(&#39;PE&#39;, 1),\n"," (&#39;PE&#39;, &#39;🐮&#39;),\n"," (&#39;India&#39;, 1),\n"," (&#39;🐮&#39;, 1),\n"," (&#39;India&#39;, 4),\n"," (&#39;India&#39;, 9),\n"," (&#39;PE&#39;, 8),\n"," (&#39;PE&#39;, 3),\n"," (&#39;India&#39;, 4),\n"," (&#39;🐮&#39;, 6),\n"," (&#39;🐮&#39;, 9),\n"," (&#39;🐮&#39;, 5)]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[72]: [(&#39;PE&#39;, 1),\n (&#39;PE&#39;, &#39;🐮&#39;),\n (&#39;India&#39;, 1),\n (&#39;🐮&#39;, 1),\n (&#39;India&#39;, 4),\n (&#39;India&#39;, 9),\n (&#39;PE&#39;, 8),\n (&#39;PE&#39;, 3),\n (&#39;India&#39;, 4),\n (&#39;🐮&#39;, 6),\n (&#39;🐮&#39;, 9),\n (&#39;🐮&#39;, 5)]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["x.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"90c9fd89-90b9-4f32-824d-c36f9fdda12d","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">🐮 [1, 6, 9, 5]\n","PE [1, &#39;🐮&#39;, 8, 3]\n","India [1, 4, 9, 4]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">🐮 [1, 6, 9, 5]\nPE [1, &#39;🐮&#39;, 8, 3]\nIndia [1, 4, 9, 4]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["y = x.groupByKey()\n","for t in y.collect():\n","    print(t[0], [v for v in t[1]])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"34f7b166-1266-4353-a574-c57aa2da2654","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>reduceByKey: </strong> Retorna los elementos reducidos por la llave. Tiene como ventaja que cada llave es reducida primero localmente en cada partición, esto lo hace más eficiente.<p><img src='https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png' alt='RDD Linage' height='280' width='442'></p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>reduceByKey: </strong> Retorna los elementos reducidos por la llave. Tiene como ventaja que cada llave es reducida primero localmente en cada partición, esto lo hace más eficiente.<p><img src='https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png' alt='RDD Linage' height='280' width='442'></p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>reduceByKey: </strong> Retorna los elementos reducidos por la llave. Tiene como ventaja que cada llave es reducida primero localmente en cada partición, esto lo hace más eficiente.<p><img src='https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png' alt='RDD Linage' height='280' width='442'></p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1132deb1-88c1-4bf4-9678-dab22f620380","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[75]: [(&#39;two&#39;, 2),\n"," (&#39;eight&#39;, 1),\n"," (&#39;four&#39;, 1),\n"," (&#39;nine&#39;, 1),\n"," (&#39;ten&#39;, 1),\n"," (&#39;one&#39;, 1),\n"," (&#39;five&#39;, 1),\n"," (&#39;six&#39;, 2)]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[75]: [(&#39;two&#39;, 2),\n (&#39;eight&#39;, 1),\n (&#39;four&#39;, 1),\n (&#39;nine&#39;, 1),\n (&#39;ten&#39;, 1),\n (&#39;one&#39;, 1),\n (&#39;five&#39;, 1),\n (&#39;six&#39;, 2)]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["words = [\"one\",\"two\",\"two\",\"four\",\"five\",\"six\",\"six\",\"eight\",\"nine\",\"ten\"]\n","data = sc.parallelize(words).map(lambda w : (w,1))\n","data2 = data.reduceByKey(lambda accum, n: accum + n)\n","data2.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0b5eab16-750a-431c-9fbd-4b0488b74908","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["data = [(\"a\", 3), (\"b\", 4), (\"a\", 1),(\"a\", 3), (\"b\", 4), (\"a\", 1)]\n","d1 = sc.parallelize(data).reduceByKey(lambda x, y: x + y).collect() # Parámetro de paralelismo por defecto\n","d2 = sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10).collect()  # TIP: optimización de ser necesario"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e85c32f1-1bdc-4b00-aef0-dcfb79ec31e5","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">[(&#39;a&#39;, 8), (&#39;b&#39;, 8)]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">[(&#39;a&#39;, 8), (&#39;b&#39;, 8)]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["print(d1)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fbdbe22a-b4b4-49d4-851e-04e5d4308ceb","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Colesce: </strong> Permite disminuir la cantidad de particiones, para optimizar recursos.<p><img src='http://www.iteblog.com/pic/pyspark/images/pyspark-page64.svg' alt='RDD Linage' height='280' width='442'></p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Colesce: </strong> Permite disminuir la cantidad de particiones, para optimizar recursos.<p><img src='http://www.iteblog.com/pic/pyspark/images/pyspark-page64.svg' alt='RDD Linage' height='280' width='442'></p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Colesce: </strong> Permite disminuir la cantidad de particiones, para optimizar recursos.<p><img src='http://www.iteblog.com/pic/pyspark/images/pyspark-page64.svg' alt='RDD Linage' height='280' width='442'></p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f2ed780a-4731-4ba8-a643-37484f40a7e8","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[79]: 3</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[79]: 3</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#rdd.partitions.size() en Scala\n","rdd1 = sc.parallelize([\"jan\",\"feb\",\"mar\",\"april\",\"may\",\"jun\"],3)\n","rdd1.getNumPartitions()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"704bb2d0-ee39-4f47-aafc-8901eb9c608e","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[80]: 2</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[80]: 2</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["rdd1 = sc.parallelize([\"jan\",\"feb\",\"mar\",\"april\",\"may\",\"jun\"],3)\n","result = rdd1.coalesce(2)\n","result.getNumPartitions()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c9292b84-b1ab-4735-991d-38633cf3f82a","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>Acciones</h2> <p>Son los gatilladores de una transformación que retorna valores. Evalua el 'RDD lineage graph'.</p><p> <img src='https://image.slidesharecdn.com/mapreducevsspark-150512052504-lva1-app6891/95/map-reduce-vs-spark-15-638.jpg?cb=1431408380' alt='RDD Linage' height='250' width='400'></p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>Acciones</h2> <p>Son los gatilladores de una transformación que retorna valores. Evalua el 'RDD lineage graph'.</p><p> <img src='https://image.slidesharecdn.com/mapreducevsspark-150512052504-lva1-app6891/95/map-reduce-vs-spark-15-638.jpg?cb=1431408380' alt='RDD Linage' height='250' width='400'></p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>Acciones</h2> <p>Son los gatilladores de una transformación que retorna valores. Evalua el 'RDD lineage graph'.</p><p> <img src='https://image.slidesharecdn.com/mapreducevsspark-150512052504-lva1-app6891/95/map-reduce-vs-spark-15-638.jpg?cb=1431408380' alt='RDD Linage' height='250' width='400'></p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2629e840-d660-4a10-a89b-b4e3d77906f4","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Collect: </strong> Retorna el todal del RDD. Considerar que todo se cargará en el driver.<p><img src='https://image.slidesharecdn.com/stratasj-everydayimshuffling-tipsforwritingbettersparkprograms-150223113317-conversion-gate02/95/everyday-im-shuffling-tips-for-writing-better-spark-programs-strata-san-jose-2015-23-638.jpg?cb=1427111079' alt='RDD Linage' height='380' width='442'></p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Collect: </strong> Retorna el todal del RDD. Considerar que todo se cargará en el driver.<p><img src='https://image.slidesharecdn.com/stratasj-everydayimshuffling-tipsforwritingbettersparkprograms-150223113317-conversion-gate02/95/everyday-im-shuffling-tips-for-writing-better-spark-programs-strata-san-jose-2015-23-638.jpg?cb=1427111079' alt='RDD Linage' height='380' width='442'></p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Collect: </strong> Retorna el todal del RDD. Considerar que todo se cargará en el driver.<p><img src='https://image.slidesharecdn.com/stratasj-everydayimshuffling-tipsforwritingbettersparkprograms-150223113317-conversion-gate02/95/everyday-im-shuffling-tips-for-writing-better-spark-programs-strata-san-jose-2015-23-638.jpg?cb=1427111079' alt='RDD Linage' height='380' width='442'></p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"19de94a4-537a-430e-9b95-1f0568ebb478","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[83]: [(&#39;b&#39;, (2, 7)), (&#39;c&#39;, (3, 3)), (&#39;c&#39;, (3, 8)), (&#39;A&#39;, (1, 4)), (&#39;A&#39;, (1, 6))]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[83]: [(&#39;b&#39;, (2, 7)), (&#39;c&#39;, (3, 3)), (&#39;c&#39;, (3, 8)), (&#39;A&#39;, (1, 4)), (&#39;A&#39;, (1, 6))]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["data = sc.parallelize([('A',1),('b',2),('c',3)])\n","data2 =sc.parallelize([('A',4),('A',6),('b',7),('c',3),('c',8)])\n","result = data.join(data2)\n","result.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6246889f-c266-482a-8382-b1f01b2075e3","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Take: </strong> Retorna los n elementos de un RDD. De manera aleatoria<p></p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Take: </strong> Retorna los n elementos de un RDD. De manera aleatoria<p></p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Take: </strong> Retorna los n elementos de un RDD. De manera aleatoria<p></p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b8ce1d7a-bfc2-49a5-badc-bc120a00e0fa","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[85]: [(&#39;b&#39;, (2, 7)), (&#39;c&#39;, (3, 3))]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[85]: [(&#39;b&#39;, (2, 7)), (&#39;c&#39;, (3, 3))]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["data = sc.parallelize([('k',5),('s',3),('s',4),('p',7),('p',5),('t',8),('k',6)],3)\n","group = data.groupByKey().collect()\n","result.take(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"91d805a4-7947-411b-b27c-c5be207cc469","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Top: </strong><p> Retorna los primeros elementos de un RDD</p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Top: </strong><p> Retorna los primeros elementos de un RDD</p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Top: </strong><p> Retorna los primeros elementos de un RDD</p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"464671f2-4ca6-4d2c-9533-b4c4c7542327","showTitle":false,"title":""}},"outputs":[],"source":["data = [1000,-100,1,23,100,-4,45,78,10000]\n","tops = sc.parallelize(data)\n","tops.top(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b7fe4971-246e-4a15-8673-3723cb97ebf7","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>countByValue: </strong><p> Retorna cuantas veces un elemento se encuentra en un RDD</p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>countByValue: </strong><p> Retorna cuantas veces un elemento se encuentra en un RDD</p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>countByValue: </strong><p> Retorna cuantas veces un elemento se encuentra en un RDD</p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"32da8c1f-9327-4042-ae89-35b755e2c85c","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[88]: defaultdict(int, {1: 1, 23: 1, 100: 2, -4: 1, 45: 3})</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[88]: defaultdict(int, {1: 1, 23: 1, 100: 2, -4: 1, 45: 3})</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["data = [1,23,100,-4,45,45,45,100]\n","result= sc.parallelize(data)\n","result.countByValue()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6b811966-6187-47c6-8536-3f4950ee5039","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>Reduce </strong><p> Toma dos elementos como entrada y le aplica una función que tiene como salida el mismo tipo de data de entrada.</p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>Reduce </strong><p> Toma dos elementos como entrada y le aplica una función que tiene como salida el mismo tipo de data de entrada.</p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>Reduce </strong><p> Toma dos elementos como entrada y le aplica una función que tiene como salida el mismo tipo de data de entrada.</p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"31c4199d-a5d2-4506-9105-273546a59b2d","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">172\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">172\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["rdd1 = sc.parallelize([20,32,45,62,8,5])\n","suma = rdd1.reduce(lambda x,y: x+y)\n","print(suma)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6e8fca20-4987-4899-8f7a-41064b0bea9f","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>¿Qué es Lazy Evaluation? </strong><p> Cuando esperar hasta el último momento sirve.</p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>¿Qué es Lazy Evaluation? </strong><p> Cuando esperar hasta el último momento sirve.</p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>¿Qué es Lazy Evaluation? </strong><p> Cuando esperar hasta el último momento sirve.</p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ba68f72f-39cf-47cd-95cf-5c8c056ebece","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<strong>¿Spark Job? </strong><p> Representa un conjunto de transformaciones disparadas por una acción, este puede ser monitoreado en el SparkUI</p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<strong>¿Spark Job? </strong><p> Representa un conjunto de transformaciones disparadas por una acción, este puede ser monitoreado en el SparkUI</p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<strong>¿Spark Job? </strong><p> Representa un conjunto de transformaciones disparadas por una acción, este puede ser monitoreado en el SparkUI</p>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4fafe747-e610-43ae-a013-591cda90242b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://10.172.247.142:48962\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.0.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[8]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Databricks Shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.247.142:48962\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\">Out[92]: </div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["spark"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9de6cf76-a26d-44e3-a03b-33b1b1b63cda","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<h2>¿Cómo se utiliza Spark en los principales proveedores Cloud?</h2> <p>AWS Elastic Map Reduce</p> <p>Google Cloud Dataproc</p> <p>Azure Databricks/HDInsight</p>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<h2>¿Cómo se utiliza Spark en los principales proveedores Cloud?</h2> <p>AWS Elastic Map Reduce</p> <p>Google Cloud Dataproc</p> <p>Azure Databricks/HDInsight</p>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":"<div class=\"ansiout\"></div>","type":"htmlSandbox"}},"output_type":"display_data"}],"source":["displayHTML(\"<h2>¿Cómo se utiliza Spark en los principales proveedores Cloud?</h2> <p>AWS Elastic Map Reduce</p> <p>Google Cloud Dataproc</p> <p>Azure Databricks/HDInsight</p>\")"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"introduction_spark","notebookOrigID":2071018292808219,"widgets":{}},"kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.5"},"vscode":{"interpreter":{"hash":"949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"}}},"nbformat":4,"nbformat_minor":0}
